{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory|\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_dir = \"G:/Mon Drive/\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [ 5.,  5.,  5.,  5.,  5.],\n",
       "       [10., 10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "\n",
    "class WeightedCategoricalCrossentropy(CategoricalCrossentropy):\n",
    "\n",
    "    def __init__(self, cost_mat=np.ones((5, 5)), name='weighted_categorical_crossentropy', **kwargs):\n",
    "        assert(cost_mat.ndim == 2)\n",
    "        assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
    "\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "\n",
    "        return super().__call__(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
    "        )\n",
    "        \n",
    "    def get_config(self):\n",
    "        return super().get_config().copy().update(\n",
    "            {'cost_mat': self.cost_mat}\n",
    "        )\n",
    "    #def get_config(self):\n",
    "    #    return {'cost_mat': self.cost_mat}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "       return cls(**config)\n",
    "\n",
    "\n",
    "def get_sample_weights(y_true, y_pred, cost_m):\n",
    "    num_classes = len(cost_m)\n",
    "\n",
    "    y_pred.shape.assert_has_rank(2)\n",
    "    assert(y_pred.shape[1] == num_classes)\n",
    "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
    "\n",
    "    y_pred = K.one_hot(K.argmax(y_pred), num_classes)\n",
    "\n",
    "    y_true_nk1 = K.expand_dims(y_true, 2)\n",
    "    y_pred_n1k = K.expand_dims(y_pred, 1)\n",
    "    cost_m_1kk = K.expand_dims(cost_m, 0)\n",
    "\n",
    "    sample_weights_nkk = cost_m_1kk * y_true_nk1 * y_pred_n1k\n",
    "    sample_weights_n = K.sum(sample_weights_nkk, axis=[1, 2])\n",
    "\n",
    "    return sample_weights_n\n",
    "\n",
    "\n",
    "# Categorical accuracy:\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "\n",
    "class WeightedCategoricalAccuracy(CategoricalAccuracy):\n",
    "\n",
    "    def __init__(self, cost_mat=np.ones((5, 5)), name='weighted_categorical_accuracy', **kwargs):\n",
    "        assert(cost_mat.ndim == 2)\n",
    "        assert(cost_mat.shape[0] == cost_mat.shape[1])\n",
    "\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cost_mat = K.cast_to_floatx(cost_mat)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "\n",
    "        return super().update_state(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            sample_weight=get_sample_weights(y_true, y_pred, self.cost_mat),\n",
    "        )\n",
    "# model.compile(metrics=[WeightedCategoricalAccuracy(cost_matrix), ...], ...)\n",
    "\n",
    "# Matriz de costos: \n",
    "import numpy as np\n",
    "w_array = np.ones((5, 5))\n",
    "# If >1 - it penalize Class i classified as j\n",
    "i = 0\n",
    "j = 1 \n",
    "w_array[i, j] = 1\n",
    "# Overall enphasis in class i:\n",
    "w_array[1, :] = 5 #10\n",
    "w_array[2, :] = 5 #5\n",
    "w_array[3, :] = 10 #29\n",
    "w_array[4, :] = 10 #37\n",
    "cost_mat = w_array\n",
    "cost_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sensibility and Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_class_result(new_model, thisclass=1):\n",
    "    selection_v1_1 = Y_valid[:,thisclass]==1\n",
    "    x = X_valid[selection_v1_1]\n",
    "    y_oh =  Y_valid[selection_v1_1]\n",
    "    y = np.argmax(y_oh, axis = 1)\n",
    "    p_pre = new_model.predict(x, batch_size = 10)\n",
    "    y_pre = np.argmax(p_pre, axis = 1)\n",
    "    if np.sum(y_pre==thisclass)==0:\n",
    "        tpr=0\n",
    "    else:\n",
    "        tpr = np.sum(y_pre==y)/np.sum(y_pre==thisclass)\n",
    "    sens = np.sum(y_pre==y)/np.sum(y==thisclass)\n",
    "\n",
    "    return(np.round(sens*100,1),np.round(tpr*100,1))\n",
    " \n",
    "\n",
    "def eval_class_result_all2(new_model):\n",
    "    \n",
    "    print(\"class | sensibility | precision\")\n",
    "    \n",
    "    x = X_valid\n",
    "    y_oh =  Y_valid\n",
    "    y = np.argmax(y_oh, axis = 1)\n",
    "    p_pre = new_model.predict(x, batch_size = 10)\n",
    "    y_pre = np.argmax(p_pre, axis = 1)\n",
    "    \n",
    "    for thisclass in range(5):\n",
    "        if np.sum(y_pre==thisclass)==0:\n",
    "            tpr=0\n",
    "        else:\n",
    "            tpr = np.sum(np.logical_and(y_pre==y, y_pre==thisclass))/np.sum(y_pre==thisclass)\n",
    "        sens = np.sum(np.logical_and(y_pre==y, y==thisclass))/np.sum(y==thisclass)\n",
    "        print((thisclass, np.round(sens*100,1),np.round(tpr*100,1)))\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def eval_class_result_all(new_model):\n",
    "    print(\"sens | true-pos-rate\")\n",
    "    print(eval_class_result(new_model, thisclass=0))\n",
    "    print(eval_class_result(new_model, thisclass=1))\n",
    "    print(eval_class_result(new_model, thisclass=2))\n",
    "    print(eval_class_result(new_model, thisclass=3))\n",
    "    print(eval_class_result(new_model, thisclass=4))\n",
    "\n",
    "    return True\n",
    "\n",
    "    #:::::::::::::::::::::::::::::::::::\n",
    "\n",
    "def eval_class_result3(Y_valid, Y_pred_valid, thisclass):\n",
    "    \n",
    "    print(\"class | sens | true-pos-rate\")\n",
    "    y_oh =  Y_valid\n",
    "    y = np.argmax(y_oh, axis = 1)\n",
    "    #p_pre = new_model.predict(x, batch_size = 10)\n",
    "    y_pre = Y_pred_valid #np.argmax(p_pre, axis = 1)\n",
    "    \n",
    "    for thisclass in range(5):\n",
    "        if np.sum(y_pre==thisclass)==0:\n",
    "            tpr=0\n",
    "        else:\n",
    "            tpr = np.sum(np.logical_and(y_pre==y, y_pre==thisclass))/np.sum(y_pre==thisclass)\n",
    "        sens = np.sum(np.logical_and(y_pre==y, y==thisclass))/np.sum(y==thisclass)\n",
    "        print((thisclass, np.round(sens*100,1),np.round(tpr*100,1)))\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "def eval_class_result_all3(Y_valid, Y_pred_valid):\n",
    "    \n",
    "    print(\"class | sens | true-pos-rate\")\n",
    "    \n",
    "    x = X_valid\n",
    "    y_oh =  Y_valid\n",
    "    y = np.argmax(y_oh, axis = 1)\n",
    "    y_pre = Y_pred_valid\n",
    "    \n",
    "    for thisclass in range(5):\n",
    "        if np.sum(y_pre==thisclass)==0:\n",
    "            tpr=0\n",
    "        else:\n",
    "            tpr = np.sum(np.logical_and(y_pre==y, y_pre==thisclass))/np.sum(y_pre==thisclass)\n",
    "        sens = np.sum(np.logical_and(y_pre==y, y==thisclass))/np.sum(y==thisclass)\n",
    "        print((thisclass, np.round(sens*100,1),np.round(tpr*100,1)))\n",
    "    \n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate credible intervals to asses stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_distrib(new_model, Niter = 100, sample_size = 200):\n",
    "    #Niter = 100\n",
    "    losses = []\n",
    "    for i in range(Niter):\n",
    "        # random sample:\n",
    "        #sample_size = 200\n",
    "        selection_v1_1 = np.random.randint(0,Y_valid.shape[0], size=sample_size)\n",
    "        x = X_valid[selection_v1_1]\n",
    "        y_oh =  Y_valid[selection_v1_1]\n",
    "        y = np.argmax(y_oh, axis = 1)\n",
    "        p_pre = new_model.predict(x, batch_size = 10)\n",
    "        y_pre = np.argmax(p_pre, axis = 1)\n",
    "        y_pre_oh = np.zeros((y_pre.size, 5))\n",
    "        y_pre_oh[np.arange(y_pre.size),y_pre] = 1\n",
    "        # Calculate loss:\n",
    "        kappa_loss = tfa.losses.WeightedKappaLoss(num_classes=5, weightage=\"quadratic\")\n",
    "        aux = kappa_loss(tf.constant(y_oh), tf.constant(y_pre_oh)).numpy()\n",
    "        losses.append(-aux)\n",
    "        \n",
    "        IC95 = np.quantile(losses, [0.025, 0.975])\n",
    "        median = np.quantile(losses, [0.5])\n",
    "        mean = np.mean(losses)\n",
    "\n",
    "    print(\"WeightedKappaLoss: Median + IC95\")\n",
    "    print(median)\n",
    "    print(IC95)\n",
    "\n",
    "    return True\n",
    "\n",
    "def eval_model_distrib3(Y_valid, Y_pred_valid, Niter = 100, sample_size = 200):\n",
    "    #Niter = 100\n",
    "    losses = []\n",
    "    for i in range(Niter):\n",
    "        # random sample:\n",
    "        #sample_size = 200\n",
    "        selection_v1_1 = np.random.randint(0,Y_valid.shape[0], size=sample_size)\n",
    "        y_oh =  Y_valid[selection_v1_1]\n",
    "        y = np.argmax(y_oh, axis = 1)\n",
    "        y_pre = Y_pred_valid[selection_v1_1]\n",
    "        y_pre_oh = np.zeros((y_pre.size, 5))\n",
    "        y_pre_oh[np.arange(y_pre.size), y_pre] = 1\n",
    "        # Calculate loss:\n",
    "        kappa_loss = tfa.losses.WeightedKappaLoss(num_classes=5, weightage=\"quadratic\")\n",
    "        aux = kappa_loss(tf.constant(y_oh), tf.constant(y_pre_oh)).numpy()\n",
    "        losses.append(-aux)\n",
    "\n",
    "        IC95 = np.quantile(losses, [0.025, 0.975])\n",
    "        median = np.quantile(losses, [0.5])\n",
    "        mean = np.mean(losses)\n",
    "\n",
    "    print(\"WeightedKappaLoss: Median + IC95\")\n",
    "    print(median)\n",
    "    print(IC95)\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data 320x320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size1=320\n",
    "im_size2=320\n",
    "X_valid = np.load('x_modeltesting_'+str(im_size1)+'.npy', mmap_mode='r')\n",
    "Y_valid = np.load('y_modeltesting_'+str(im_size1)+'.npy', mmap_mode='r') \n",
    "Y_valid = np.array(Y_valid, np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos EfficientNetB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMSize=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 11s 61ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 100.0, 73.1)\n",
      "(1, 0.0, 0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 0.0, 0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[-9.53673862e-07]\n",
      "[-9.53673862e-07 -9.53673862e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = \"EfficientNetB3_v3softmax.12--11.72\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 100\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 61s 430ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 96.0, 74.9)\n",
      "(1, 2.6, 4.6)\n",
      "(2, 0.0, 0)\n",
      "(3, 10.1, 10.8)\n",
      "(4, 0.0, 0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.17540984]\n",
      "[-0.01072994  0.46349899]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = \"EfficientNetB0_Augm_im150.18--0.59\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMSize=320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 100.0, 73.2)\n",
      "(1, 0.0, 0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0.0)\n",
      "(4, 0.0, 0)\n",
      "Median + IC95\n",
      "[1.74779737]\n",
      "[-0.80786069 13.81551054]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = \"EfficientNetB3_v3_im320softmax.08--11.24\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "eval_class_result_all2(new_model)\n",
    "eval_model_distrib(new_model, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 50s 352ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 99.7, 73.3)\n",
      "(1, 0.0, 0)\n",
      "(2, 0.0, 0.0)\n",
      "(3, 1.8, 18.2)\n",
      "(4, 5.8, 33.3)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[-9.53673862e-07]\n",
      "[-0.03090974  0.23553222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RELU 1 \n",
    "modelname = \"EfficientNetB3_v3_im320.01--0.62\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU 1 \n",
    "modelname = \"EfficientNetB3_v3_im320.26--0.58\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "eval_class_result_all3(new_model)\n",
    "eval_model_distrib3(new_model, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 733s 11s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 99.4, 73.7)\n",
      "(1, 0.0, 0.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 17.4, 26.8)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.10385771]\n",
      "[-0.03975178  0.3868664 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" : RELU epoch 2S\n",
    "modelname = \"EfficientNetB3_v3_im320.05--0.43\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=64)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 661s 10s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 100.0, 73.1)\n",
      "(1, 0.0, 0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 0.0, 0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[-9.53673862e-07]\n",
      "[-9.53673862e-07 -9.53673862e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" : RELU epoch 2S\n",
    "modelname = \"EfficientNetB3_v3_im320.02--6.48\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=64)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 678s 18s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 99.2, 74.3)\n",
      "(1, 0.3, 1.3)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 11.6, 38.5)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.11282661]\n",
      "[-0.0054557   0.40786014]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.10--0.50\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=120)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 745s 5s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.9, 74.6)\n",
      "(1, 0.6, 2.1)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 25.0)\n",
      "(4, 4.7, 33.3)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.12280444]\n",
      "[-0.00344978  0.37578494]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.30--0.58\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 743s 5s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.9, 74.6)\n",
      "(1, 0.6, 2.1)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 22.2)\n",
      "(4, 3.5, 33.3)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.12536908]\n",
      "[-0.01075495  0.36435149]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.35--0.60\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 748s 5s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.5)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 3.7, 19.0)\n",
      "(4, 3.5, 37.5)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.11162785]\n",
      "[-0.00641648  0.34319431]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.45--0.60\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMSize=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 85s 594ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 83.2, 74.5)\n",
      "(1, 0.0, 0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 50.0, 5.3)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.09512078]\n",
      "[-0.07185967  0.30013445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Ultimo modelo con efficientNetB0 (256 N+16N)\n",
    "modelname = \"EfficientNetB0_Augm_im150.04--0.10\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 8s 56ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 92.2, 76.4)\n",
      "(1, 4.2, 3.1)\n",
      "(2, 1.0, 28.0)\n",
      "(3, 0.0, 0)\n",
      "(4, 32.6, 36.8)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.31717581]\n",
      "[0.05951617 0.68567601]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo de los iniciales.\n",
    "modelname = \"EfficientNetB0.53--0.55\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 128s 916ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.2, 74.0)\n",
      "(1, 1.0, 2.9)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 8.1, 25.0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.07636344]\n",
      "[-0.02837352  0.28922281]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_Augm_im224.34--0.59\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 224\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 9s 56ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 78.8, 80.9)\n",
      "(1, 21.7, 7.8)\n",
      "(2, 9.5, 33.2)\n",
      "(3, 0.0, 0)\n",
      "(4, 50.0, 21.7)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.5484539]\n",
      "[0.16418184 0.93576103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_lrscheduled.90--0.54_im320\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 9s 55ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 60.9, 82.1)\n",
      "(1, 17.3, 5.8)\n",
      "(2, 25.6, 25.7)\n",
      "(3, 0.0, 0)\n",
      "(4, 59.3, 12.5)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.44098826]\n",
      "[0.19381849 0.77395185]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_lrscheduled_wsample.155--0.49\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model(x, training=False)\n",
    "finalmodel = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Model sent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 12s 57ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.54878539]\n",
      "[0.23858967 0.91320414]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_lrscheduled.86--0.53_im320\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"EfficientNetB0_lrscheduled_.06--0.58_im320\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 9s 57ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 86.1, 78.0)\n",
      "(1, 12.8, 5.5)\n",
      "(2, 3.6, 39.3)\n",
      "(3, 0.0, 0)\n",
      "(4, 19.8, 27.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.35385622]\n",
      "[0.08395287 0.66618527]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_lrscheduled2_.02--0.58_im320\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 57s 354ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.5)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 3.7, 17.4)\n",
      "(4, 3.5, 33.3)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.10094289]\n",
      "[-0.00599269  0.30333924]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.47--0.62\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 59s 388ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.6)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 26.1)\n",
      "(4, 4.7, 36.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.12694184]\n",
      "[-0.01702057  0.37412501]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.68--0.65\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.68--0.65\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 59s 388ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.6)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 25.0)\n",
      "(4, 4.7, 40.0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.1266206]\n",
      "[-0.00632462  0.39654783]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB3_v3_im320.70--0.60\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 99.3, 73.6)\n",
      "(1, 0.0, 0.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.9, 5.6)\n",
      "(4, 9.3, 25.8)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.04785122]\n",
      "[-0.03685377  0.34171506]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"EfficientNetB0_Augm_im224.06--0.51_im320\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 734s 5s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.9, 74.5)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 4.6, 21.7)\n",
      "(4, 4.7, 44.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.116194]\n",
      "[-0.00612102  0.38696356]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo Ensamblado\n",
    "modelname = \"EfficientNetB3_v3_im320.82--0.66\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Ensamblados w Automatic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 439s 3s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.6)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 26.1)\n",
      "(4, 4.7, 36.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.1277698]\n",
      "[-0.00588814  0.3914008 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo con activación \"relu\" \n",
    "modelname = \"ModelEns1_im320320.03--0.34\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/EnsembledModel/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 55s 336ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.8, 74.6)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 5.5, 26.1)\n",
      "(4, 4.7, 36.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.11638458]\n",
      "[-0.00577723  0.40232876]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo Ensamblado1\n",
    "modelname = \"ModelEns1_im320320.01--0.36\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/EnsembledModel/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Ensamblado1\n",
    "modelname = \"ModelEns1_im320320.04--0.33\"\n",
    "new_model = tf.keras.models.load_model(base_dir + 'models/EnsembledModel/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = new_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Ensamblados w Manual weigthing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 56s 376ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 92.9, 77.8)\n",
      "(1, 7.0, 6.9)\n",
      "(2, 5.7, 37.3)\n",
      "(3, 0.0, 0)\n",
      "(4, 37.2, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.45377935]\n",
      "[0.13290523 0.83454523]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo ensamblado manualmente:\n",
    "modelname1 = \"EfficientNetB0_lrscheduled.86--0.53_im320\"\n",
    "new_model1 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "modelname2 = \"EfficientNetB3_v3_im320.82--0.66\"\n",
    "new_model2 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "\n",
    "image_width = 320\n",
    "image_height = image_width\n",
    "model_1 = new_model1\n",
    "model_1 = Model(inputs=model_1.inputs,\n",
    "                outputs=model_1.outputs,\n",
    "                name='Modelo1')\n",
    "model_2 = new_model2\n",
    "model_2 = Model(inputs=model_2.inputs,\n",
    "                outputs=model_2.outputs,\n",
    "                name='Modelo2')\n",
    "models = [model_1, model_2]\n",
    "model_input = tf.keras.layers.Input(shape=(image_width, image_height, 3))\n",
    "model_outputs = [model(model_input) for model in models]\n",
    "ensemble_output = tf.keras.layers.Average()(model_outputs)\n",
    "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
    "ensemble_model.save(base_dir + 'models/EnsembledModel/EnsMan_'+ modelname1 + \"_\" + modelname2 +'.h5')\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid = ensemble_model.predict(X_valid, verbose=1, batch_size=32)\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 250, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Weigthed average model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. First ensembled model B0 + B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 9s 56ms/step\n",
      "138/138 [==============================] - 49s 337ms/step\n"
     ]
    }
   ],
   "source": [
    "# Modelo1 ensamblado con pesos:\n",
    "modelname1 = \"EfficientNetB0_lrscheduled.86--0.53_im320\"\n",
    "new_model1 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname1 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "modelname2 = \"EfficientNetB3_v3_im320.82--0.66\"\n",
    "new_model2 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname2 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid1 = new_model1.predict(X_valid, verbose=1, batch_size=32)\n",
    "pred_valid2 = new_model2.predict(X_valid, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.54266515]\n",
      "[0.23816063 0.9060863 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid = pred_valid1*1 + pred_valid2*0\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.6, 81.0)\n",
      "(1, 23.0, 7.8)\n",
      "(2, 11.0, 32.3)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.7)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.56114239]\n",
      "[0.21882511 0.98428618]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid = pred_valid1*0.6 + pred_valid2*0.4\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Otro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 12s 55ms/step\n",
      "138/138 [==============================] - 9s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "modelname1 = \"EfficientNetB0_lrscheduled.86--0.53_im320\"\n",
    "new_model1 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname1 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "modelname2 = \"EfficientNetB0_lrscheduled_.06--0.58\"\n",
    "new_model2 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname2 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "# Convertir modelo2 para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model2(x, training=False)\n",
    "model2_final = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "pred_valid1 = new_model1.predict(X_valid, verbose=1, batch_size=32)\n",
    "pred_valid2 = model2_final.predict(X_valid, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 89.2, 77.7)\n",
      "(1, 7.7, 4.4)\n",
      "(2, 2.7, 37.5)\n",
      "(3, 0.0, 0)\n",
      "(4, 38.4, 28.2)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.41030999]\n",
      "[0.11979033 0.81239852]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 50\n",
    "weigth2 = 50\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 5000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.56257081]\n",
      "[0.32187698 0.82463332]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 100\n",
    "weigth2 = 0\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.2, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.0, 31.9)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.56102738]\n",
      "[0.33247174 0.82119677]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 95\n",
    "weigth2 = 5\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Otro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 9s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "modelname1 = \"EfficientNetB0_lrscheduled.86--0.53\"\n",
    "new_model1 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname1 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    " \n",
    "# Convertir modelo2 para que acepte input de 320\n",
    "initmodelsize = 150\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "outputs = new_model1(x, training=False)\n",
    "model1_final = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Predict and save\n",
    "pred_valid1 = model1_final.predict(X_valid, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 14s 94ms/step\n"
     ]
    }
   ],
   "source": [
    "modelname2 = \"EfficientNetB0_Augm_im224.33--0.60\"\n",
    "new_model2 = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelname2 + '.h5',\n",
    "    custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "    'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    " \n",
    "# Convertir modelo2 para que acepte input de 320\n",
    "initmodelsize = 224\n",
    "inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)\n",
    "#x = tf.keras.layers.Rescaling(scale=1/255)(x)\n",
    "outputs = new_model2(x, training=False)\n",
    "model2_final = tf.keras.Model(inputs, outputs)\n",
    "#finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelname + '_im320.h5') # if saved\n",
    "\n",
    "# Predict and save\n",
    "pred_valid2 = model2_final.predict(X_valid, verbose=1, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2142409e-15, 1.0000000e+00, 1.4532386e-30, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [1.7557878e-34, 1.0000000e+00, 1.8495453e-24, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       ...,\n",
       "       [9.9999845e-01, 1.5644404e-06, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00],\n",
       "       [1.0000000e+00, 3.5004891e-08, 3.8741978e-32, 0.0000000e+00,\n",
       "        0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_valid1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.5624668]\n",
      "[0.50311133 0.62453474]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 100\n",
    "weigth2 = 0\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.56283718]\n",
      "[0.50023053 0.62087616]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 100\n",
    "weigth2 = 0\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.2, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.56425679]\n",
      "[0.50669972 0.62739525]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth1 = 90\n",
    "weigth2 = 10\n",
    "# Obtain combination \n",
    "pred_valid = pred_valid1*weigth1/100 + pred_valid2*weigth2/100\n",
    "y_pre_valid = np.argmax(pred_valid, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble of various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  EfficientNetB0_lrscheduled.86--0.53\n",
      "138/138 [==============================] - 65s 462ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 77.0, 81.2)\n",
      "(1, 22.7, 7.5)\n",
      "(2, 11.2, 32.1)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.1)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.55950096]\n",
      "[0.50205465 0.62593307]\n",
      "Model:  EfficientNetB3_v3_im320.82--0.66\n",
      "138/138 [==============================] - 431s 3s/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.9, 74.5)\n",
      "(1, 0.6, 2.0)\n",
      "(2, 0.0, 0)\n",
      "(3, 4.6, 21.7)\n",
      "(4, 4.7, 44.4)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.13156114]\n",
      "[0.09698712 0.1617627 ]\n",
      "Model:  EfficientNetB0_Augm_im224.33--0.60\n",
      "138/138 [==============================] - 128s 920ms/step\n",
      "class | sens | true-pos-rate\n",
      "(0, 98.2, 74.0)\n",
      "(1, 1.0, 2.9)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 8.1, 25.0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.09104246]\n",
      "[0.0615885  0.12613663]\n"
     ]
    }
   ],
   "source": [
    "modelnames = [\"EfficientNetB0_lrscheduled.86--0.53\",\"EfficientNetB3_v3_im320.82--0.66\",\"EfficientNetB0_Augm_im224.33--0.60\"]\n",
    "weigths = [60,30,10]\n",
    "sizeinput = [150, 320, 224]\n",
    "\n",
    "predictions = list()\n",
    "wave_pred =  0\n",
    "for this_i in range(len(modelnames)):\n",
    "    #modelnamei = \"EfficientNetB0_lrscheduled.86--0.53\"\n",
    "    modelnamei = modelnames[this_i]\n",
    "    print(\"Model: \", modelnamei)\n",
    "    thismodeli = tf.keras.models.load_model(base_dir + 'models/PretrainedNetworks/' + modelnamei + '.h5',\n",
    "        custom_objects={'WeightedCategoricalCrossentropy': WeightedCategoricalCrossentropy(cost_mat=cost_mat),\n",
    "        'WeightedCategoricalAccuracy': WeightedCategoricalAccuracy(cost_mat=cost_mat)})\n",
    "\n",
    "    \n",
    "    # Convertir modelo2 para que acepte input de 320\n",
    "    initmodelsize = sizeinput[this_i]\n",
    "    finalmodel = thismodeli\n",
    "    if initmodelsize<320:\n",
    "        inputs = tf.keras.Input(shape=(320, 320, 3))\n",
    "        x = tf.keras.layers.Resizing(initmodelsize,initmodelsize)(inputs)        \n",
    "        outputs = thismodeli(x, training=False)\n",
    "        finalmodel = tf.keras.Model(inputs, outputs)\n",
    "    #finalmodel.save(base_dir + 'models/PretrainedNetworks/'+ modelnamei + '_im320.h5') # if saved\n",
    "\n",
    "    # Predict and save\n",
    "    this_prediction = finalmodel.predict(X_valid, verbose=1, batch_size=32)\n",
    "    predictions.append(this_prediction)\n",
    "\n",
    "    wave_pred = this_prediction*(weigths[this_i]/np.sum(weigths))\n",
    "\n",
    "    # Show results\n",
    "    y_pre_valid = np.argmax(this_prediction, axis = 1)\n",
    "    eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "    eval_model_distrib3(Y_valid, y_pre_valid, Niter = 200, sample_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 77.3, 81.1)\n",
      "(1, 23.0, 7.7)\n",
      "(2, 11.0, 32.3)\n",
      "(3, 0.0, 0)\n",
      "(4, 46.5, 24.5)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.55646187]\n",
      "[0.49778648 0.61417418]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigths = weigths\n",
    "wave_pred = 0\n",
    "for this_i in range(len(predictions)):\n",
    "    wave_pred = wave_pred + predictions[this_i]*(weigths[this_i]/np.sum(weigths))\n",
    "\n",
    "y_pre_valid = np.argmax(wave_pred, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 200, sample_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class | sens | true-pos-rate\n",
      "(0, 98.2, 74.0)\n",
      "(1, 1.0, 2.9)\n",
      "(2, 0.0, 0)\n",
      "(3, 0.0, 0)\n",
      "(4, 8.1, 25.0)\n",
      "WeightedKappaLoss: Median + IC95\n",
      "[0.08873658]\n",
      "[0.05768638 0.12445675]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigths = [0,0,100]\n",
    "wave_pred = 0\n",
    "for this_i in range(len(predictions)):\n",
    "    wave_pred = wave_pred + predictions[this_i]*(weigths[this_i]/np.sum(weigths))\n",
    "\n",
    "y_pre_valid = np.argmax(wave_pred, axis = 1)\n",
    "eval_class_result_all3(Y_valid, y_pre_valid)\n",
    "\n",
    "eval_model_distrib3(Y_valid, y_pre_valid, Niter = 1000, sample_size = 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('Py373')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e237917c5a43921db6be90f0dd69a03c2fa8e36338d3e591ab551368fa8af88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
